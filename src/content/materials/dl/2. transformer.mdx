# Transformer Architecture

## Motivation

Traditional recurrent models process sequences sequentially, limiting parallelism and long-range dependency modeling.

Transformers address these limitations using attention mechanisms.

## Self-Attention

Self-attention allows each token to attend to all other tokens in a sequence. This enables flexible modeling of contextual relationships.

Key components include:

- Query
- Key
- Value

Attention weights are computed using similarity between queries and keys.

## Multi-Head Attention

Multi-head attention allows the model to attend to information from different representation subspaces simultaneously.

This improves expressiveness and stability.

## Positional Encoding

Since transformers lack recurrence, positional information is injected using positional encodings. These encodings preserve word order information.

## Impact on NLP

Transformers form the basis of modern language models such as BERT, GPT, and T5. They have dramatically improved performance across NLP benchmarks.

## Limitations

Despite their success, transformers face challenges related to:

- Computational cost
- Data efficiency
- Reasoning robustness

These limitations motivate ongoing research in efficient and reasoning-aware architectures.
